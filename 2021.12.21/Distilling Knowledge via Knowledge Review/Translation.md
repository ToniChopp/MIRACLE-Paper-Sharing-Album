## 1
&emsp;&emsp;深度卷积神经网络（CNN）已经在各种计算机视觉任务中取得了显著的成功。然而，CNN的成功往往伴随着相当大的计算量和内存消耗，这使得它在资源有限的设备上的应用成为一个具有挑战性的课题。已经有一些技术用于训练快速和紧凑的神经网络，包括设计新的架构[10, 2, 11, 26]，网络修剪[20, 15, 34, 4, 19]，量化[13] ，以及知识提炼[9, 25]。  
&emsp;&emsp;考虑到知识提炼的实用性、效率，以及最重要的是它的潜在作用，我们在本文中重点讨论了知识蒸馏。它形成了一个非常普遍的路线，适用于几乎所有的网络架构，并可以与许多其他策略相结合，如网络剪枝和量化[32]，以进一步改善网络设计。  
&emsp;&emsp;知识蒸馏最早是在[9]中提出的。这个过程是在一个较大的网络（即老师）的监督下训练一个小的网络（也称为学生）。在[9]中，知识是通过教师的logit提炼出来的，这意味着学生是由 ground truth 标签和教师的logit监督的。最近，人们努力提高知识蒸馏的有效性。FitNet[25]通过中间特征来提炼知识。AT[38]进一步优化了FitNet并使用特征的注意力图来传递知识。PKT[23]将教师的知识建模为概率分布，而CRD[28]使用对比目标来传递知识。所有这些解决方案都集中在转换和损失函数上。  
&emsp;&emsp;**我们的新发现** 我们在本文中从一个关于教师和学生之间的连接路径的新角度来解决这个挑战性的问题。为了简单地理解我们的想法，我们首先展示了以前的工作是如何处理这些路径的。如图1(a)-(c)所示，所有以前的方法只使用同级别的信息来指导学生。例如，当监督学生的第四阶段输出时，总是利用教师的第四阶段信息。这个程序看起来很直观，很容易构建。但我们耐人寻味地发现，这其实是整个知识蒸馏框架的一个瓶颈--结构的快速更新令人惊讶地提高了许多任务的整个系统性能。  
&emsp;&emsp;我们研究了以前被忽视的在知识蒸馏中设计连接路径的重要性，并相应提出了一个新的有效框架。关键的修改是利用教师网络中的低级特征来监督学生的深层特征，从而使整体性能得到很大的提高。  
&emsp;&emsp;我们进一步分析网络结构，发现学生的高层次阶段有很大能力从教师的低层次特征中学习有用的信息。更多的分析将在第4.4节提供。这个过程类似于人类的学习曲线[35]，一个年轻的孩子只能理解所教知识的一小部分。在成长的过程中，过去几年越来越多的知识可能逐渐被理解并作为经验被记住。  
&emsp;&emsp;**我们的知识回顾框架** 基于这些发现，我们建议使用教师的多层次信息来指导学生网络的单层次学习。 我们新颖的管道如图1（d）所示，我们称之为 "知识回顾"。复习机制是使用以前的（较浅的）特征来指导当前的特征。 这意味着学生必须经常检查以前学习过的东西，以刷新对 "旧知识 "的理解和背景。在一段时间的学习中，将不同阶段所学的知识联系起来，是我们人类学习的一个普遍做法。  
&emsp;&emsp;然而，如何从教师的多层次信息中提取有用的信息，以及如何将它们转移到学生身上，是一个开放性的挑战问题。为了解决这些问题，我们提出了一个剩余学习框架，以使学习过程稳定和高效。此外，我们还设计了一个新的基于注意力的融合（ABF）模块和一个分层上下文损失（HCL）函数来提高性能。 我们提出的框架使学生网络大大提高了学习的有效性。  
&emsp;&emsp;通过应用这个想法，我们在许多计算机视觉任务中取得了更好的表现。第4节中的大量实验证明了我们提出的知识审查策略的巨大优势。  
本文的贡献在于：  
- 我们在知识蒸馏中提出了一种新的审查机制，利用教师的多层次信息来指导学生网的单层次学习。  
- 我们提出了一个剩余学习框架，以更好地实现审查机制的学习过程。  
- 为了进一步改进知识审查机制，我们提出了一个基于注意力的融合（ABF）模块和一个层次化的上下文损失（HCL）函数。  
- 通过应用我们的蒸馏框架，我们在多个计算机视觉任务中实现了许多紧凑模型的最先进性能。
## 2.
&emsp;&emsp;知识蒸馏的概念是在[9]中提出的，其中学生网络从groundtruth标签和教师提供的软标签中学习。Fit- Net[25]通过一个阶段的中间特征来提炼知识。FitNet的想法很简单，学生网络的特征通过卷积层被转移到教师的相同形状。$\mathcal{L}_2$距离被用来测量它们之间的距离。  
&emsp;&emsp;许多方法都效仿FitNet，使用单阶段特征来提炼知识。PKT[23]将教师的知识建模为一个概率分布，并使用KL发散来测量距离。RKD[22]使用多个实例关系来指导学生的学习。CRD[28]将对比性学习和知识提炼结合起来，并使用对比性目标来转移知识。  
&emsp;&emsp;还有一些方法使用多阶段信息来转移知识。AT[38]使用多层注意力图来转移知识。FSP[36]从层特征中生成FSP矩阵，并使用该矩阵来指导学生。 SP[29]进一步改进了AT。SP不使用单一的输入信息，而是使用实例之间的相似性来引导学生。OFD[8]包含一个新的距离函数，使用边际ReLU提炼出教师和学生之间的主要信息。  
&emsp;&emsp;所有以前的方法都没有讨论 "回顾知识 "的可能性，然而，在我们的工作中发现，这对快速提高系统性能非常有效。
## 3
&emsp;&emsp;我们首先正式确定了知识蒸馏过程和审查机制。然后，我们提出了一个新的框架，并引入了基于注意力的融合模块和层次化的上下文损失函数。  
### 3.1 回顾机制
&emsp;&emsp;给定一个输入图像$\mathbf{X}$和学生网络$\mathcal{S}$，我们让$\mathbf{Y}_\mathcal{S} = \mathcal{S}(\mathbf{X})$代表学生的输出对数。$\mathcal{S}$可以分成不同的部分$(\mathcal{S}_1, \mathcal{S}_2, ..., \mathcal{S}_n, \mathcal{S}_\mathcal{c})$，其中$\mathcal{S}_\mathcal{c}$是分类器，$\mathcal{S}_1, ..., \mathcal{S}_n$是由下采样层分隔的不同阶段。因此，生成输出$\mathbf{Y}_\mathcal{S}$的过程可以表示为：  
$\mathbf{Y}_\mathcal{S}=\mathcal{S}_\mathcal{c}\circ \mathcal{S}_n \circ ... \circ \mathcal{S}_1(\mathbf{X})$  
我们把"$\circ$"称为函数的嵌套，其中$g\circ f(x) = g(f(x))$。$\mathbf{Y}_\mathcal{S}$是学生的输出，中间的特征是$(\mathbf{F}_\mathcal{s}^1, ..., \mathbf{F}_\mathcal{s}^n)$。第$i$个特征的计算方法为：  
$\mathbf{F}_\mathcal{s}^i=\mathcal{S}_i \circ ... \circ \mathcal{S}_1(\mathbf{X})$  
对于教师网络$\mathcal{T}$，其过程几乎相同，我们省略细节。按照以前的记号,单层知识蒸馏可以表示为：  
$\mathcal{L}_{SKD}=\mathcal{D}(\mathcal{M}_\mathcal{s}^i(\mathbf{F}_\mathcal{s}^i),\mathcal{M}_\mathcal{t}^i(\mathbf{F}_\mathcal{t}^i)) $  
其中$\mathcal{M}$是将特征转移到注意图[38]或因素[14]的目标表示的变换。$\mathcal{D}$是衡量学生老师之间差距的距离函数。同样地，多层知识蒸馏写为：  
$\mathcal{L}_{MKD}=\sum_{i\in \mathbf{I}}\mathcal{D}(\mathcal{M}_\mathcal{s}^i(\mathbf{F}_\mathcal{s}^i),\mathcal{M}_\mathcal{t}^i(\mathbf{F}_\mathcal{t}^i)) $  
其中，$\mathbf{I}$存储的特征层用于转移知识。  
&emsp;&emsp;我们的审查机制是用以前的特征来指导当前的特征。带有审查机制的单层知识蒸馏被形式化为：  
$\mathcal{L}_{SKD\_R}=\sum_{j=1}^i \mathcal{D}(\mathcal{M}_\mathcal{s}^{i,j}(\mathbf{F}_\mathcal{s}^i),\mathcal{M}_\mathcal{t}^{j,i}(\mathbf{F}_\mathcal{t}^j)) $  
虽然乍一看，它与多层知识蒸馏法有一些相似之处，但实际上它有本质的区别。在这里，学生的特征被固定为$\mathbf{F}_\mathcal{s}^i$，而我们使用教师的前i层特征来指导$\mathbf{F}_\mathcal{s}^i$。复习机制和多层蒸馏法是互补的概念。当复习机制与多层知识蒸馏相结合时，损失函数变为：  
$\mathcal{L}_{MKD\_R}=\sum_{i\in \mathbf{I}} (\sum_{j=1}^i \mathcal{D}(\mathcal{M}_\mathcal{s}^{i,j}(\mathbf{F}_\mathcal{s}^i),\mathcal{M}_\mathcal{t}^{j,i}(\mathbf{F}_\mathcal{t}^j))) $
在我们的实验中，$\mathcal{L}_{MKD\_R}$损失只是在训练过程中与原始损失一起单独加入，推理结果与原始模型完全相同。所以我们的方法在测试时是完全没有成本的。我们使用因子来平衡蒸馏损失和原始损失。以分类任务为例，整个损失函数被定义为：  
$\mathcal{L}=\mathcal{L}_{CE}+\mathcal{L}_{MKD\_R} $  
在我们提出的审查机制中，我们只使用教师的较浅的特征来监督学生的较深的特征。我们发现，相反的做法带来的好处不多，反而浪费了很多资源。直观的解释是，更深更抽象的特征对于早期阶段的学习来说过于复杂。更多分析见第4.4节。
### 3.2 剩余学习框架
&emsp;&emsp;按照以前的工作，我们首先设计了一个简单的框架，如图2（a）所示。变换$\mathcal{M}_\mathcal{s}^{i,j}$只是由卷积层和最近插值层组成，将学生的第i个特征转移到与教师的第j个特征的大小一致。我们不对教师的特征$\mathbf{F}_t$进行转换。 学生的特征被转换为与教师特征相同的大小。
&emsp;&emsp;图2(b)显示了直接将该想法应用于多层蒸馏，并对所有阶段的特征进行蒸馏。然而，由于各阶段之间存在巨大的信息差异，这种策略并不是最优的。而且，它产生了一个复杂的过程，所有的特征都被使用。例如，一个有n个阶段的网络需要计算n(n+1)/2对关于损失函数的特征，这使得学习过程变得繁琐，并花费很多资源。  
&emsp;&emsp;为了使该程序更加可行和优雅，我们将图2(b)的公式(6)重新表述为(6)为图2(b)的公式。  
$\mathcal{L}_{MKD\_R}=\sum_{i=1}^n (\sum_{j=1}^i \mathcal{D}(\mathbf{F}_s^i,\mathbf{F}_t^j)) $  
其中为简单起见，省略了特征的变换。现在我们将i和j的两个和的顺序调换为:  
$\mathcal{L}_{MKD\_R}=\sum_{j=1}^n (\sum_{i=j}^n \mathcal{D}(\mathbf{F}_s^i,\mathbf{F}_t^j)) $  
当j是固定的，公式。(9) 累积了教师特征$\mathbf{F}_t^j$和学生特征$\mathbf{F}_s^j-\mathbf{F}_s^n$之间的距离。通过特征的融合[40，16]，我们将距离的总和近似为融合特征的距离。这就导致了  
$\sum_{i=j}^n \mathcal{D}(\mathbf{F}_s^i,\mathbf{F}_t^j)\approx\mathcal{D}(\mathcal{u}(\mathbf{F}_s^j, ..., \mathbf{F}_s^n), \mathbf{F}_t^j) $  
其中$u$是一个融合特征的模块。图2(c)说明了这种近似的结构现在更加有效。但融合的计算可以进一步逐步优化，如图2（d）所示，以获得更高的效率。$\mathbf{F}_s^j, ..., \mathbf{F}_s^n$的融合是由$\mathbf{F}_s^j$和$u(\mathbf{F}_s^{j+1}, ..., \mathbf{F}_s^n))$组合计算的，其中融合操作递归定义为$u(.,.)$，应用于连续的特征图。将$\mathbf{F}_s^{j+1,n}$表示为$\mathbf{F}_s^{j+1}$到$\mathbf{F}_s^n$的特征融合，损失写为：  
$\mathcal{L}_{MKD\_R}=\mathcal{D}(\mathbf{F}_s^n,\mathbf{F}_t^n)+\sum_{j=n-1}^1\mathcal{D}(u(\mathbf{F}_s^{j}, \mathbf{F}_s^{j+1,n}),\mathbf{F}_t^j) $  
这里我们从n-1向下循环到1，利用$\mathbf{F}_s^{j+1,n} \cdot \mathbf{F}_s^{n,n}=\mathcal{M}_s^{n,n}(\mathbf{F}_s^n) $。详细结构如图2(d)所示，其中ABF和HCL分别是为这个结构设计的融合模块和损失函数。它们的细节将在第3.3节讨论。  
&emsp;&emsp;图2(d)中的结构是优雅的，它利用剩余学习的概念简化了蒸馏过程。 例如，阶段4的学生特征与阶段3的学生特征聚集在一起，模仿阶段3的教师特征。因此，阶段4的学生特征学习了阶段3的学生和老师之间的特征的残余。残留信息很可能是教师产生更高质量结果的关键因素。  
&emsp;&emsp;这种剩余的学习过程比直接让学生从教师的低层次特征中学习的高级特征更稳定和有效。通过残差学习框架，学生的高级特征可以更好地逐步提取有用的信息。 此外，利用公式（11），我们消除了求和，将总的复杂性降低到n对距离。  
### 3.3 ABF 和 HCL
&emsp;&emsp;图2(d)中有两个关键部分。它们是基于注意力的融合（ABF）和层次化的上下文损失（HCL）。我们在此对它们进行解释。  
&emsp;&emsp;图3(a)。高层次的特征首先被调整为与低层次特征相同的形状。然后，来自不同层次的两个特征被串联在一起，生成两个H×W 注意力图。这些图分别与两个特征相乘。最后，这两个特征相加产生最终输出。  
&emsp;&emsp;ABF模块可以根据输入特征生成不同的注意图。因此，两个特征图可以动态地进行汇总。自适应汇总比直接汇总更好，因为两个特征图来自网络的不同阶段，它们的信息是多样的。低级和高级特征可能集中在不同的分区。 注意力图可以更合理地聚合它们。 更多的实验结果包括在4.4节。  
&emsp;&emsp;HCL的细节如图3(b)所示。通常情况下，我们使用$\mathcal{L}_2$距离作为两个特征图之间的损失函数。$\mathcal{L}_2$距离可以有效地在同一层次的特征之间传递信息。但在我们的框架中，不同层次的信息被聚合在一起，以向老师学习。琐碎的全局$\mathcal{L}_2$距离不足以转移复合层次的信息。  
&emsp;&emsp;受[41]的启发，我们提出了HCL，利用空间金字塔集合，将知识的转移分离成不同层次的背景信息。通过这种方式，信息在不同的抽象层次中被更好地蒸馏出来。其结构非常简单：我们首先利用空间金字塔集合法从特征中提取不同层次的知识，然后利用$\mathcal{L}_2$距离在它们之间分别进行蒸馏。 尽管结构简单，但HCL适合我们的框架。更多的实验结果显示在第4.4节。  
## 4
&emsp;&emsp;我们对各种任务进行了实验。首先，我们将我们的方法与其他关于分类的知识提炼方法进行比较。我们对不同的设置、不同的结构和数据集进行了实验。同时，我们将我们的方法应用于对象检测和实例分割任务。我们的方法也以较大的幅度持续改进基线模型。
### 4.1 分类
**Datasets** (1) CIFAR-100 contains 50K training images
with 0.5K images per class and 10K test images. (2) ImageNet
[3] is the most challenging dataset for classification,
which provides 1.2 million images for training and 50K images
for validation over 1,000 classes.  
&emsp;&emsp;**实现细节** 在CIFAR-100数据集上，我们实验了不同的代表性网络架构，包括VGG[27]、ResNet[7]、WideResNet[37]、MobileNet[26]和ShuffleNet[39, 21]。我们使用与[28]相同的训练设置，除了线性放大初始学习率和按照[5]设置批次大小。  
&emsp;&emsp;具体来说，我们对所有模型进行240个历时的训练，在前150个历时之后，每30个历时的学习率衰减0.1。我们将MobileNet和ShuffleNet的学习率初始化为0.02，其他模型为0.1。所有模型的批次大小为128。我们对所有模型进行三次训练，并报告平均精度。为了公平起见，以前的方法结果要么在以前的论文中报告（当训练设置与我们相同时），要么使用作者发布的代码与我们的训练设置获得。  
&emsp;&emsp;在ImageNet上，我们使用标准的训练过程，对模型进行100个历时的训练，每30个历时的学习率就会衰减。我们将学习率初始化为0.1，并将批次大小设置为256。  
&emsp;&emsp;**CIFAR-100的结果** **表1**总结了CIFAR-100的结果，教师和学生有相同风格的架构。我们将以前的方法根据它们使用的特征分成不同的组。KD是唯一使用对数的方法。FitNet组的方法使用单层信息，而AT组的方法使用多层信息。我们的方法采用了多层特征与审查机制。它在所有的架构上都优于之前的所有方法。  
&emsp;&emsp;我们还试验了学生和老师有不同的建筑风格的设置，并在**表2**中显示了结果。OFD的方法[8]和我们的方法使用多层蒸馏。他们的表现优于那些从最后一层进行蒸馏的方法，这表明我们的知识审查机制成功地放松了以前强调的中间层或最后一层的蒸馏条件[28]。  
&emsp;&emsp;**ImageNet上的结果** CIFAR-100中的图像数量很少。所以我们也在ImageNet上进行了实验，以验证我们方法的可扩展性。我们分别从ResNet50到MobileNet[11]，以及从ResNet34到ResNet18进行了两种提炼设置的实验。如**表3**所示，我们的方法再次优于所有其他方法。设置(a)由于结构的不同而具有挑战性。 但是我们的方法的优势始终是突出的。在设置（b）中，学生和老师之间的差距已经被以前的最佳方法减少到一个非常小的数值2.14。我们进一步将其减少到1.70，实现了20%的相对性能改进。
### 4.2 目标检测
&emsp;&emsp;我们还将我们的方法应用于其他计算机视觉任务。 在物体检测上，与分类任务的程序一样，我们在学生和教师的骨干输出特征之间进行提炼。更多细节在补充文件中介绍。我们使用有代表性的COCO2017数据集[18]来评估我们的方法，并将最流行的开源报告Detectron2[33]作为我们的强基线。我们使用Detrctron2提供的最佳预训练模型作为老师。学生模型是按照传统[31]使用标准训练策略进行训练。所有的性能都被在COCO2017验证集上评估。我们对两阶段和单阶段方法都进行了实验。  
&emsp;&emsp;由于只有少数方法[31, 8]被声称可用于检测，我们转载了流行的方法[9, 25]和最新的方法[31]。比较结果见**表4**。我们注意到，知识提炼方法，如KD和FitNet，也提高了检测的性能。但是收益是有限的。FGFI[31]是直接为检测而设计的，在这个任务上比其他方法效果更好。不过，我们的方法还是以很大的优势超过了它。  
&emsp;&emsp;在两阶段方法FasterRCNN[24]中，我们改变了骨干架构。同一风格的架构之间的知识提炼使ResNet18和ResNet50的mAP分别提高了3.49和2.43。 它们是重要的数字。ResNet50和MobileNetV2之间的蒸馏仍然将基线从29.47提升到33.71。在单级检测器RetinaNet[17]上，学生和老师之间的差距很小，我们的方法也将mAP提高了2.33。在具有挑战性的物体检测任务上的成功表明了我们方法的通用性和有效性。  
### 4.3 实例分割
&emsp;&emsp;在本节中，我们将我们的方法应用于更具挑战性的实例分割任务。据我们所知，这是知识提炼方法第一次应用于实例分割。我们还使用了Detectron2[33]提供的强基线。我们把Mask R-CNN[6]作为我们的模型，并在不同的骨干架构之间进行提炼。这些模型在COCO2017的训练集上进行训练，并在验证集上进行了评估。结果显示在**表5**中。  
&emsp;&emsp;我们的方法也明显提高了实例分割任务的性能。对于同一风格的架构之间的蒸馏，我们将ResNet18和ResNet50的性能提高了2.37和1.74，并将教师和学生之间的差距相对减少了32%和51%。 即使对于不同风格的架构的蒸馏，我们也比MobileNetV2好3.19。  
&emsp;&emsp;我们的方法在所有的图像分类、物体检测和实例分割任务中都表现得很好，并且完成了所有的SOTA结果，这表明我们的方法具有显著的功效和适用性。  
## 4.4 更多分析
&emsp;&emsp;**跨阶段的知识蒸馏** 我们分析了跨阶段知识转移的有效性。我们将ResNet20作为学生，ResNet56作为CIFAR-100数据集的老师。在ResNet20和ResNet56中有四个阶段。我们选择学生的不同阶段和教师的不同阶段来监督他们。结果总结在**表6**中。  
&emsp;&emsp;这些结果表明，从教师那里提炼出学生的同阶段信息是最好的解决方案。 这与我们的直觉是一致的。此外，耐人寻味的是，来自较低层次的信息也是有帮助的。但是，从教师的高层提炼对学生的训练有不利的影响。  
&emsp;&emsp;这表明，较深阶段的学生有能力从较低阶段的教师那里学到有用的信息。反之，对于早期阶段的学生来说，来自教师的更深层次和更抽象的特征过于复杂。这与我们的理解和我们提出的审查机制是一致的，即利用教师的浅层阶段来监督学生的深层阶段。  
&emsp;&emsp;**消融研究** 进行消融实验，在实验中逐一加入消融成分以测量其效果。结果在**表7**中总结了准确度和方差。我们在CIFAR100数据集上使用WRN16-2作为学生，WRN40-2作为教师。 基线是用学生和教师的同一水平的特征之间的$\mathcal{L}_2$距离来训练的。  
&emsp;&emsp;通过我们提出的复习机制，结果比基线有所提高，如第二行所示，它使用了图2(b)中所示的三段式结构。当我们用残差学习框架进一步细化结构时，学生产生了更大的收益。基于注意力的融合模块和层次化的上下文损失函数在单独利用时也提供了很大的改进。而当我们把它们集合在一起时，就会得到最好的结果。 令人惊讶的是，它们甚至比老师还要好。  
## 5
&emsp;&emsp;在本文中，我们从一个新的角度研究了知识蒸馏，并相应地提出了审查机制，即用教师中的多层来监督学生中的一层。与之前所有的SOTA相比，我们的方法在所有的分类、物体检测和实例分割任务上都取得了一致的显著改善。我们只使用阶段性的输出，并且在总体上已经取得了不错的结果。  
&emsp;&emsp;对于未来的工作，我们还将在一个阶段内采用特征。此外，在我们的框架中还将研究其他损失函数。  
