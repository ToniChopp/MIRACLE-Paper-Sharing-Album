# <center>Anchor-guided online meta adaptation for fast one-Shot instrument segmentation from robotic surgical videos</center>
#### <p align="right">By Zhen Huang</p>
## 1. Introduction
《基于anchor引导的在线元适应算法的机器人手术视频仪器快速一次性分割》。论文lib：https://doi.org/10.1016/j.media.2021.102240。 

​		机器人辅助手术(RAS)中带注释的手术数据的缺乏，促使以往的研究借鉴相关领域知识，通过适应性的方法对手术图像实现有前景的分割结果。对于机器人手术视频中密集的仪器跟踪，在术前准备期间收集一个初始场景来指定目标仪器(或工具的部分)是可取的和可行的。在本文中，我们研究了具有挑战性的机器人手术视频的一次性器械分割，在测试时只提供每个视频的第一帧蒙版，这样预先训练的模型(从容易获取的来源学习)可以适应目标器械。直接的方法通过对每个给定掩码的模型进行微调来传递领域知识。这样的一次优化需要上百次迭代，并且测试运行时是不可行的。为此，我们提出了锚引导的在线元适应(AOMA)方法。我们通过元学习实现了快速的一次测试时间优化，从源视频中获得了良好的模型初始化和学习率，避免了费力的手工微调。在具有匹配感知损失的特定视频任务空间中优化可训练的两个组件。此外，我们设计了一个锚引导的在线适应，以解决整个机器人手术序列的性能下降。该模型在锚点匹配支持的运动不敏感伪掩模上连续适配。AOMA在两种实际场景下取得了最先进的结果:(1)普通视频到手术视频，(2)公开手术视频到内部手术视频，同时大大减少了测试运行时间。



## 2. 知识整理

### 2.1Domain generalisation

域泛化，也称DG。是迁移学习中的一种方法，它研究的问题是从若干个具有不同数据分布的数据集（领域）中学习一个泛化能力强的模型，以便在 「未知 (Unseen)」 的测试集上取得较好的效果，

**domain adaptation：**领域自适应。是迁移学习中的一种方法。旨在利用源域中标注好的数据，学习一个精确的模型，运用到无标注或只有少量标注的目标域中。本质上是一种数据增强的迁移方法 [1]。

**主要区别：**

**Domain Adaptation（DA）：**需要源域（train set）和目标域（test set）都有数据。

**Domain Generalization（DG）**：只需要源域（train set）

DG有两个主要的难点：

1.目标域不可见（unseen）  

2.对任何目标域都有作用。

按照我个人的理解，“任何”二字应该还是有局限的，比如说源域为cv中的图像分类任务，目标域（target domain）也应该是类似的任务而不能是NLP任务，所以在这里的“任何”并不是广义上的任何。 
 　　目前成熟的domain generalization方法基本可以分为三类，分别是： 
 　　1.feature-based method
 　　2.classifier-based method 
 　　3.instance-reweighting method 



### 2.2 Guided Anchoring

​		 Anchor机制是目前主流的（也有不基于anchor的算法， 比如CornerNet和GridNet）基于深度学习的目标检测算法的重要基石。目前的最先进的检测器都是使用的密集的anchor模式，即在图片上均匀放置好一组定义好形状和大小的anchor。

​    anchor的设计一般有两个要求**：**1.alignment，为了用卷积特征作为anchor的表示，anchor的中心需要和特征图的像素中心比较好地对齐；2.consistency，不同位置对应的anchor的形状和大小应该一致。

   这篇paper的研究表明anchor机制可以实施得更加有效和高效。本文提出的方法Guided  Anchoring利用语义特征来指导anchor。方法会联合预测感兴趣物体中心可能出现的位置和在不同位置的形状大小。也就是先预测anchor的形状大小及位置，再在这些anchor的基础上预测目标。

​							    ![img](.\src\1.png)   

   从图中看到**Guided anchoring模块**中有两个部分：**anchor generation和feature adaption**。首先是anchor generation，对于**第一部分anchor location prediction**，采用网络N_L。N_L对输入特征图F_I使用1x1的卷积，得到与F_I相同分辨率的输出，该输出的位置(i,j)对应原图上((i+0.5)s,(j+0.5)s)，其中s是这个F_I的stride。N_L得到的输出的每个位置的值表示原图I上对应位置出现物体的可能性（当然得使用sigmoid）。

   anchor generation的**第二部分是anchor shape prediction**，采用1x1的卷积网络N_S，输入F_I，输出和F_I尺寸相同的2通道的特征图，表示每个位置可能的最好的anchor的尺寸。文章发现直接训练输出绝对尺寸不稳定，因为绝对尺寸的范围太大了，因此采用下式
																 ![在这里插入图片描述](.\src\2.png)

​		s是stride，σ是一个尺度系数（文中取8），N_S只需要预测dw和dh，通过这样的方法将训练目标的范围从约[0,1000]缩小到了[-1,1]。
     接下来是anchor-guided feature  adaption。由于每个位置的anchor形状不一样，所以不能再像之前的基于anchor的方法那样，直接利用F_I进行1x1的卷积预测每个anchor的结果，而应该对feature  map进行adaption，也就是大一点的anchor对应的感受野应该大一点，小一点的anchor对应的感受野应该小一点，于是作者想到用可变形卷积的思想。先对每个位置预测一个卷积的offset（1x1卷积，输入为shape prediction），然后根据该offset field进行3x3的可变形卷积就完成了对feature map的adaption。

​		训练时除了基本的分类损失和回归损失以外，guided anchor方法还需要学习anchor location和anchor shape，因此还有两个额外的损失函数：
​													 ![在这里插入图片描述](.\src\3.png)

### 2.3 Meta-Learning

这是本文所重点提到的概念，个人之前没有接触过，查阅了相关文献进行了一定的了解。

> 元学习（Meta-Learing），又称“学会学习“（Learning to learn）, 即利用以往的知识经验来指导新任务的学习，使网络具备学会学习的能力，是解决小样本问题（Few-shot Learning）常用的方法之一

这里面有几个点是需要注意的：

​		1.**元学习中的元，即meta的含义**：meta-learning的本质是增加学习器在multi-task中的泛化能力，其对于数据和任务都需要采样。因此最终学习到的F(x)可以在unseen的task中迅速建立起对应的mapping，达到“学会学习“的目的。具体的，”meta“体现在网络对于每个task的学习，不断适应具体的task，使得我们的网络最终具备一种抽象的学习能力。

​		2.**meta-learning中的train和test**：训练过程定义为”meta-training“，测试过程则定义为”meta-testing“。具体如下图：

<img src="C:\Users\Hz\AppData\Roaming\Typora\typora-user-images\image-20211109231943970.png" alt="image-20211109231943970" style="zoom: 33%;" />

其训练和测试过程都需要两类数据集（Support set & Query set）。

构建S和Q主要还是采用randomly choose的模式进行随机选出N个类，再按照类别随机选出sample。

训练则通常采用一种称为Episodic Training的方法。

​		3.**和迁移学习的区别**：如paper中所述，迁移学习和元学习都是本文设计到的重点。所以有必要了解一下二者的difference&connection。目标上看，二者都是为了增加学习器在multi-task中的泛化能力。但meta-learning更偏重于task和data的双重采样。for instance，对于一个N分类的task，meta-learning只会建立一个N/2的classifier，每轮train的episode都能被视为一个子任务，学习到的F(x) 则可以帮助其在unseen task中建立mapping。相比之下，迁移学习更多强调从A任务到B任务的能力迁移，较为不强调任务空间的概念。



### 2.4 Online Adaptation

​		由于关注的对象的外观会随时间变化，并且会出现新的背景对象，因此引入一种在线适应方案来适应这些变化。在对目标对象进行预训练时，进入场景的新对象会造成问题，因为它们从未被用作负面训练示例，被分配的可能性很高。

​		online adaptation的基本思想：使用具有非常可靠预测的像素作为训练示例。选择预测前景概率超过某个阈值α的像素作为正例。重要的是适应能够保留积极类别的记忆，以便为增加的许多负面例子创造平衡。实验表明，若忽略这一步骤会在前景蒙板上产生空洞。

​		作者最初以相同的方式选择负面训练样例，即使用前景概率非常低的像素。然而，这可能会导致性能下降，因为在大的外观变化过程中，假阴性像素将被选为负面训练示例，从而有效地摧毁了适应这些变化的所有机会。因此，基于两帧之间的移动很小的假设，以不同的方式选择负面训练示例。这个想法是选择离最后预测的对象掩码很远的所有像素。为了处理噪音，可以通过erosion操作缩小最后一个掩膜，实验中，作者使用大小为15的正方形结构元素，但发现此参数的确切值并不重要。之后，计算一个距离变换，该变换为每个像素提供距离掩模最近的前景像素的欧几里得距离。最后，应用阈值d并将距离大于d的所有像素视为负面示例。

​		既未标记为正面也未标为负面例子的像素被分配了“不关心”标签，并且在线更新期间被忽略。我们现在可以在当前帧上微调网络，因为每个像素都有一个用于训练的标签。然而，在实践中，发现使用获得的训练样例进行微调很快就会导致漂移。为了避免这个问题，建议在第一帧中作为在线更新期间的附加训练样例，因为对于第一帧，groundtruth是可用的。作者发现为了获得好的结果，第一帧应该比当前帧更频繁地采样，即在在线适应期间，我们每帧执行总共非线性更新步骤，其中在当前帧上仅执行n行，并且其余的是在第一帧上执行的。此外，我们将当前帧的损失权重降低β因子（例如β≈0.05）。值为0.05可能看起来小得惊人，但必须记住第一帧经常用于更新，快速导致更小的梯度，而当前帧仅被选择几次。

​		在线自适应期间，根据前一帧的掩码选择否定训练示例。因此，可能发生像素被选作负面的例子，并且它被同时预测为前景。称这些像素为不利底片。发生硬阴性的常见情况是当先前看不见的物体远离感兴趣的物体进入场景时，这通常会被网络检测为前景。我们发现从下一帧中使用的前景蒙版中移除难以确定否定训练示例的难题很有帮助。此步骤允许再次选择下一帧中的负片作为反面示例。此外，我们试图通过增加更新步骤的数量和/或在存在严重负面情况下当前帧的损失范围来更强调网络以适应硬性负面情况。但是，这并没有进一步改善结果。

​		除了前面描述的步骤之外，作者还提出了一个简单的启发式方法，它可以使我们的方法更好地抵抗像遮挡这样的困难：如果（在可选侵蚀之后）最后假定的前景蒙版上没有任何东西，我们假设感兴趣的对象丢失并且不要应用任何在线更新，直到网络再次找到非空的前景蒙板。


## 3.实验结果

![image-20220129114105462](.\src\4.png)

Base：不调整直接用于训练（no-metaTraining）。

Base-FT：加入fine-tuning。

CRN，Dual MF分别采用前一帧和光流的映射作为pseudo scalar，对运动较为敏感。

最终，在两种实际场景下(1)普通视频到手术视频和(2)公开手术视频到内部手术视频都取得了很好的分割效果，证明了其方法的有效性和适用性，同时大大减少了测试运行时间。它还显示了其他密集跟踪任务(如工具提示跟踪)的巨大潜力。



## 4. 总结
如前面所述，本文的贡献在于提出了anchor-guided online meta adaptation (AOMA)。

通过元学习实现了快速的一次测试时间优化，从源视频中获得了良好的模型初始化和学习率，避免了费力的手工微调。在具有匹配感知损失的特定视频任务空间中优化可训练的两个组件。

此外，设计了anchor-guided online adaptation，来解决整个机器人手术序列的性能下降。该模型在anchor matching支持的motion-insensitive pseudo-masks上都能很好适用。AOMA在两种实际场景下取得了最先进的结果:(1)普通视频到手术视频，(2)公开手术视频到内部手术视频，同时大大减少了测试运行时间。

主要贡献在于：

1.解决了机器人手术视频的一次性仪器分割问题，只需每个视频的第一帧掩模即可快速适配。

2.设计一种锚引导的在线自适应模型，连续地对锚匹配生成的第一帧掩码和随后的伪掩码进行自适应。基于运动不敏感的在线监护可以很好地解决机器人手术视频中存在的快速器械运动问题。

3.提出通过匹配感知优化过程元学习最优模型初始化和学习速度，实现快速online adaptation。



**Expectation：**1.进行online adaptation时不对整个模型更新，只对一些层更新，收敛的更快。

2.对部分效果不好的pseudo scalar（noise）采用过滤/剪枝等方法。

